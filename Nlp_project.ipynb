{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fahri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/fahri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fahri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/fahri/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fahri/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pyLDAvis as pyLDAvis\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "stopwords.words('english')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7W1kWnAoEfJd",
    "outputId": "996090a9-59b9-4529-a518-3dc6fc215ea3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#reading files\n",
    "path = \"/Users/fahri/Desktop/review_polarity/txt_sentoken/neg\"\n",
    "\n",
    "list_ = []\n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "        list_.append(data)\n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "\n",
    "        # call read text file function\n",
    "        read_text_file(file_path)\n",
    "\n",
    "\n",
    "df1= pd.DataFrame({'Content':list_})\n",
    "path = \"/Users/fahri/Desktop/review_polarity/txt_sentoken/pos\"\n",
    "list_ = []\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "\n",
    "        # call read text file function\n",
    "        read_text_file(file_path)\n",
    "\n",
    "\n",
    "df2= pd.DataFrame({'Content':list_})"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xKMaP-M9EfJe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#concatenation of neg and pos\n",
    "frames = [df1,df2]\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "#for train test copy this column\n",
    "dfTrain = pd.DataFrame()\n",
    "dfTrain = df.copy()\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hNnR4DkQEfJf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             Content\n0  bad . bad . \\nbad . \\nthat one word seems to p...\n1  isn't it the ultimate sign of a movie's cinema...\n2   \" gordy \" is not a movie , it is a 90-minute-...\n3  disconnect the phone line . \\ndon't accept the...\n4  when robert forster found himself famous again...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bad . bad . \\nbad . \\nthat one word seems to p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>isn't it the ultimate sign of a movie's cinema...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\" gordy \" is not a movie , it is a 90-minute-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disconnect the phone line . \\ndon't accept the...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>when robert forster found himself famous again...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = []\n",
    "for i in range(2000):\n",
    "    sent_list.append(df['Content'][i])\n",
    "df_sent = pd.DataFrame({'Content': sent_list})\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "K989eolQEfJf",
    "outputId": "e8b9ebce-904e-4eb9-b353-c754d9bbe61a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             Content\n0  bad . bad .  bad .  that one word seems to pre...\n1  isn't it the ultimate sign of a movie's cinema...\n2   \" gordy \" is not a movie , it is a 90-minute-...\n3  disconnect the phone line .  don't accept the ...\n4  when robert forster found himself famous again...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bad . bad .  bad .  that one word seems to pre...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>isn't it the ultimate sign of a movie's cinema...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\" gordy \" is not a movie , it is a 90-minute-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>disconnect the phone line .  don't accept the ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>when robert forster found himself famous again...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'] = df['Content'].str.replace('\\n', ' ')  #remove new line characters\n",
    "df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zTg2zYzpEfJf",
    "outputId": "9f3415ec-db4f-4eb4-d961-9c289b376468"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1037950616 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2000\u001B[39m):\n\u001B[1;32m     18\u001B[0m     sentence \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent\u001B[39m\u001B[38;5;124m'\u001B[39m][i]\n\u001B[0;32m---> 19\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     word_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(doc)):\n",
      "File \u001B[0;32m~/Desktop/pythonProject/venv/lib/python3.8/site-packages/spacy/language.py:1014\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m    993\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    994\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    995\u001B[0m     text: Union[\u001B[38;5;28mstr\u001B[39m, Doc],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    998\u001B[0m     component_cfg: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Dict[\u001B[38;5;28mstr\u001B[39m, Any]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    999\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Doc:\n\u001B[1;32m   1000\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001B[39;00m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;124;03m    is preserved.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1012\u001B[0m \u001B[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001B[39;00m\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1014\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1015\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m component_cfg \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1016\u001B[0m         component_cfg \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/Desktop/pythonProject/venv/lib/python3.8/site-packages/spacy/language.py:1105\u001B[0m, in \u001B[0;36mLanguage._ensure_doc\u001B[0;34m(self, doc_like)\u001B[0m\n\u001B[1;32m   1103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m doc_like\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m-> 1105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_doc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_like\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[1;32m   1107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Doc(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab)\u001B[38;5;241m.\u001B[39mfrom_bytes(doc_like)\n",
      "File \u001B[0;32m~/Desktop/pythonProject/venv/lib/python3.8/site-packages/spacy/language.py:1094\u001B[0m, in \u001B[0;36mLanguage.make_doc\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1088\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001B[39;00m\n\u001B[1;32m   1089\u001B[0m \n\u001B[1;32m   1090\u001B[0m \u001B[38;5;124;03mtext (str): The text to process.\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;124;03mRETURNS (Doc): The processed doc.\u001B[39;00m\n\u001B[1;32m   1092\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length:\n\u001B[0;32m-> 1094\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1095\u001B[0m         Errors\u001B[38;5;241m.\u001B[39mE088\u001B[38;5;241m.\u001B[39mformat(length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(text), max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length)\n\u001B[1;32m   1096\u001B[0m     )\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(text)\n",
      "\u001B[0;31mValueError\u001B[0m: [E088] Text of length 1037950616 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.tokens import Doc\n",
    "def stringTolist(s):\n",
    "\n",
    "    # initialize an empty string\n",
    "    str1 = \"\"\n",
    "    # return string\n",
    "    return (str1.join(s))\n",
    "\n",
    "def pre_process_text(doc,j):\n",
    "    # Generate a new list of tokens here\n",
    "    new_words = doc[j].lemma_\n",
    "    new_doc = Doc(doc.vocab, words=new_words)\n",
    "    return new_doc\n",
    "\n",
    "for i in range(2000):\n",
    "    sentence = df['Content'][i]\n",
    "    doc = nlp(sentence)\n",
    "    word_list = []\n",
    "\n",
    "    for j in range(len(doc)):\n",
    "\n",
    "\n",
    "        if str(doc[j]) != str(doc[j].lemma_):\n",
    "            a = stringTolist(str(pre_process_text(doc,j)).replace(\" \",\"\"))\n",
    "\n",
    "\n",
    "            #print(counter)\n",
    "           # print(f\"{str(token):>20} : {str(token.lemma_)}\")\n",
    "        else :\n",
    "            a = stringTolist(str(doc[j]))\n",
    "\n",
    "        word_list.append(a)\n",
    "\n",
    "    df['Content'][i] = word_list\n",
    "\n",
    "df['Content'].head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "U-fRN9OKEfJf",
    "outputId": "99e412c1-73e7-4e22-9175-d27a4e5994f1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df['Content'] = df['Content'].apply(lambda x: ''.join(TextBlob(x).correct()))\n",
    "# too long to run"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7qhwdaU6EfJf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "df['Content'] = df['Content'].apply(lambda x: reduce_lengthening(str(x)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b2Bbeu9UEfJf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for i in range(2000):\n",
    "    df['Content'][i] = re.sub(r'[^a-zA-Z]', ' ', str(df['Content'][i])) # punctuation and numbers\n",
    "    df['Content'][i] = re.sub(r'^\\s*|\\s\\s*', ' ', df['Content'][i]).strip().lower() #double whitespace\n",
    "\n",
    "    df['Content'][i] = re.split(\"\\W+\", df['Content'][i] ) #tokenize\n",
    "    df['Content'][i] = [word for word in df['Content'][i] if word not in stop_words] # stop words\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IREp8iLFEfJf",
    "outputId": "492d7527-6519-43af-f188-8d518dbbe061"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "for i in range(2000):\n",
    "    sent = df['Content'][i]\n",
    "    sent = \" \".join(w for w in nltk.wordpunct_tokenize(str(sent)) if w.lower() in words or not w.isalpha())\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "EUC8957OEfJg",
    "outputId": "b947cb87-278c-4feb-ebbb-92a75e0a5161"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['number_of_words'] = df['Content'].apply(lambda x : len(TextBlob(str(x)).words))\n",
    "\n",
    "df['polarity'] = df['Content'].apply(lambda x : TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "df['subjectivity'] = df['Content'].apply(lambda x : TextBlob(str(x)).sentiment.subjectivity)\n",
    "\n",
    "df['language'] = df['Content'].apply(lambda x : detect(str(x)))\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TWpit3ZnEfJg",
    "outputId": "a01f022c-29db-4d67-ff51-172219780b90"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    "\n",
    "    # initialize an empty string\n",
    "    str1 = \" \"\n",
    "\n",
    "    # return string\n",
    "    return (str1.join(s))\n",
    "\n",
    "corpus = []\n",
    "for i in range(2000):\n",
    "    a = listToString(df['Content'][i])\n",
    "    corpus.append(a)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VuutKye0EfJg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "bag_of_words_model = CountVectorizer()\n",
    "bag_of_word_df = pd.DataFrame(bag_of_words_model.fit_transform(corpus).todense())\n",
    "bag_of_word_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "\n",
    "bag_of_word_df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_ro88wA9EfJg",
    "outputId": "5a81ee81-53e3-4b93-ce0b-4cd5968d2ab2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_of_words_model_small = CountVectorizer(max_features=10)\n",
    "bag_of_word_df_small = pd.DataFrame(bag_of_words_model_small.fit_transform(corpus).todense())\n",
    "bag_of_word_df_small.columns = sorted(bag_of_words_model_small.vocabulary_)\n",
    "\n",
    "bag_of_word_df_small.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yi5FTFtVEfJg",
    "outputId": "a0a5255a-cdfa-488f-b18d-8080968c2819"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(corpus).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "\n",
    "tfidf_df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dNrlms0XEfJg",
    "outputId": "e956d7d7-2e53-4642-975c-b0a29e280bfe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_model_small = TfidfVectorizer(max_features=10)\n",
    "tfidf_df_small = pd.DataFrame(tfidf_model_small.fit_transform(corpus).todense())\n",
    "tfidf_df_small.columns = sorted(tfidf_model_small.vocabulary_)\n",
    "\n",
    "tfidf_df_small.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rvDfv2loEfJg",
    "outputId": "05450642-0e16-4f06-a9d6-021dabf5859d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidf_df_small.head"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PjaVb2tBEfJg",
    "outputId": "89369ac9-682f-430b-a170-1a831d25b508"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_of_words_model_small2 = CountVectorizer(max_features=50)\n",
    "bag_of_word_df_small2 = pd.DataFrame(bag_of_words_model_small2.fit_transform(corpus).todense())\n",
    "bag_of_word_df_small2.columns = sorted(bag_of_words_model_small2.vocabulary_)\n",
    "\n",
    "bag_of_word_df_small2.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "c7mA8wgLEfJg",
    "outputId": "1a3d14a6-76e5-41d5-bc1d-49723d970732"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words50 = {}\n",
    "\n",
    "for i in range(50):\n",
    "    words50[((bag_of_word_df_small2.columns)[i])] = sum(bag_of_word_df_small2[((bag_of_word_df_small2.columns)[i])])\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(background_color=\"blue\",width=2000,height=1000, max_words=50,relative_scaling=0.5).generate_from_frequencies(words50)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yhq2mCjmEfJh",
    "outputId": "a891b655-a988-4bfc-f61d-b96dce97d717"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# initialize kmeans with 5 centroids\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "# fit the model\n",
    "kmeans.fit(bag_of_word_df)\n",
    "# store cluster labels in a variable\n",
    "clusters = kmeans.labels_"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "vs_YxYfqEfJh",
    "outputId": "2740f53a-cb20-4db7-87fe-820ac490e9fe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# initialize PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# pass our X to the pca and store the reduced vectors into pca_vecs\n",
    "pca_vecs = pca.fit_transform(bag_of_word_df)\n",
    "# save our two dimensions into x0 and x1\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "u4RrvO8HEfJh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# assign clusters and pca vectors to our dataframe\n",
    "df['cluster'] = clusters\n",
    "\n",
    "df['x0'] = x0\n",
    "df['x1'] = x1"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "02-e8mbbEfJh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "3SmbMxeAEfJh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfTrain['Content'] =  df['Content'].apply(lambda x: ','.join(map(str, x)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ov2fKMSuEfJh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xk = dfTrain['Content']\n",
    "yk = df['cluster']\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xk,yk,test_size = 0.2,random_state = 6)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5Fu2MeVUEfJh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "clfk = Pipeline([('vect', tfidf_model), ('clf', KMeans(n_clusters=5))])\n",
    "\n",
    "clfk.fit(X_train, y_train)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "vLxWEo5iEfJh",
    "outputId": "306e69c2-e449-425a-d05e-69d2542cbbc1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check accuracy, precision,recall\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "print(classification_report(y_test, clfk.predict(X_test)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xRRZoSGkEfJh",
    "outputId": "f5a76390-064c-49d5-84a8-3581088437a5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# set image size\n",
    "plt.figure(figsize=(12, 7))\n",
    "# set a title\n",
    "plt.title(\"TF-IDF + KMeans \", fontdict={\"fontsize\": 15})\n",
    "# set axes names\n",
    "plt.xlabel(\"X0\", fontdict={\"fontsize\": 15})\n",
    "plt.ylabel(\"X1\", fontdict={\"fontsize\": 15})\n",
    "# create scatter plot with seaborn, where hue is the class used to group the data\n",
    "sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette=\"Set2\")\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6neqcuj9EfJh",
    "outputId": "ee7f6e78-04d5-47e3-9472-dae67a3e1eed"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xg_o5TO2EfJh",
    "outputId": "4ed740f0-68de-4e6a-acea-55d64d1fb4be"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "is_emotional = []\n",
    "\n",
    "for i in range(2000):\n",
    "    if df['polarity'][i] < 0:\n",
    "        sentiment.append(0)\n",
    "    else:\n",
    "        sentiment.append(1)\n",
    "    if df['subjectivity'][i] < 0.5:\n",
    "        is_emotional.append(0)\n",
    "    else:\n",
    "        is_emotional.append(1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lGXMU555EfJi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['sentiment'] = sentiment\n",
    "df['is_emotional'] = is_emotional\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "mq7zjKyQEfJi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dfTrain['Content']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 6)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PDQceaB4EfJi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Connect LogisticRegression and TfidfVectorizer by Pineline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline([('vect', tfidf_model), ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9GOHfo7YEfJi",
    "outputId": "0d097c58-a527-419c-b559-041a2c595e1e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check accuracy, precision,recall\n",
    "\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "M95QJmfSEfJj",
    "outputId": "c2037476-4777-4bee-9d4c-931a9ea4fc79"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment_model evaluation by ROC Curves\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn import metrics\n",
    "\n",
    "fig = plt.figure(1, figsize = (5, 5))\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8pFLVrH-EfJj",
    "outputId": "82d6ce04-6038-41ca-8153-9daca591c93c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Linear SVC\n",
    "classifier_svc = Pipeline([('tfidf',TfidfVectorizer()),\n",
    "('clf',LinearSVC())])\n",
    "\n",
    "classifier_svc.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, classifier_svc.predict(X_test)))\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2Lopmqz6EfJj",
    "outputId": "fea516e5-8e3e-4d6e-ff92-d08e720366a3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dfTrain['Content']\n",
    "y = df['is_emotional']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GBmYfDgCEfJj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Random Forest\n",
    "classifier_rf = Pipeline([('tfidf',TfidfVectorizer()), ('clf',RandomForestClassifier(bootstrap= False, criterion= 'entropy', n_estimators= 100))])\n",
    "classifier_rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GcQJ1qXuEfJj",
    "outputId": "a633ada0-2a4d-4367-eade-e9d9d03c6deb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(y_test, classifier_rf.predict(X_test)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "49-WfFPiEfJj",
    "outputId": "59979caf-2b0c-4936-c5dd-28ccd3007f93"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = (5, 5))\n",
    "y_pred_proba = classifier_rf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4kUTbJ3EEfJj",
    "outputId": "d3ccd8c6-03ef-4486-aa8e-2df1c6484f90"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_wLrp6EuEfJj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenized = df['Content']\n",
    "dictionary = corpora.Dictionary(tokenized)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "corpus2 = [dictionary.doc2bow(tokens) for tokens in tokenized]\n",
    "print(corpus2[:1])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "mePM9GYOEfJj",
    "outputId": "e7095675-4345-491e-9e26-8507ee64455a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus2[:1]]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "D3vx5W4eEfJj",
    "outputId": "cae7aef5-991e-493a-85f6-2cf63045fcd3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#LDA\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus2, num_topics = 5, id2word=dictionary, passes=15)\n",
    "ldamodel.save('mOdel.gensim')\n",
    "topics = ldamodel.print_topics(num_words=30)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Mu-g7TMGEfJj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_document_topics = ldamodel.get_document_topics(corpus2[6])\n",
    "print(get_document_topics)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IQSBxUzSEfJj",
    "outputId": "7011ab02-8ea6-4f2e-a7aa-6297cddac62f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hLoCdA98EfJj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualizing topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "lda_viz = gensim.models.ldamodel.LdaModel.load('mOdel.gensim')#load lda model\n",
    "\n",
    "lda_display = gensimvis.prepare(lda_viz, corpus2, dictionary, sort_topics=True)\n",
    "pyLDAvis.display(lda_display)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DLZk_nWQEfJj",
    "outputId": "852837fc-2943-4bde-c9ec-3c243bc4e175"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame()\n",
    "df_neg = pd.DataFrame()\n",
    "\n",
    "df_neg['Content'] = df_sent['Content'].iloc[0:1000]\n",
    "df_pos['Content'] = df_sent['Content'].iloc[1001:2000]\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aP66OQ7hEfJj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_string = \"\"\n",
    "\n",
    "for i in range(10):\n",
    "    neg_string += df_neg['Content'][i]\n",
    "\n",
    "neg_string"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Sua-tYaxEfJj",
    "outputId": "b3e20b4a-1cfe-4a2b-b022-ab52e591ec87"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos_string = \"\"\n",
    "\n",
    "for i in range(1001,1010):\n",
    "    pos_string += df_pos['Content'][i]\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ruf20MqQEfJj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos_string"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "REtvIn74EfJj",
    "outputId": "6eb405af-7b3a-47d4-ff05-ae74ccae4385"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def summarize(text, n):\n",
    "    # Split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # Tokenize and vectorize each sentence\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "    # Compute the similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = cosine_similarity(vectors[i].reshape(1, -1), vectors[j].reshape(1, -1))[0,0]\n",
    "    # Run TextRank\n",
    "    sentence_ranks = textrank(similarity_matrix)\n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentences = sorted(((score, index) for index, score in enumerate(sentence_ranks)), reverse=True)\n",
    "    # Extract the top-ranked sentences\n",
    "    top_n_ranked_sentences = [ranked_sentences[i] for i in range(n)]\n",
    "    top_n_ranked_sentences.sort(key=lambda x: x[1])\n",
    "    # Extract the summary\n",
    "    summary = \" \".join([sentences[index] for score, index in top_n_ranked_sentences])\n",
    "    return summary\n",
    "\n",
    "def textrank(matrix):\n",
    "    # Add a small constant to the matrix to avoid division by zero\n",
    "    matrix += 1e-9\n",
    "    # Compute the transition matrix\n",
    "    transition_matrix = matrix / matrix.sum(axis=1)[:, None]\n",
    "    # Initialize the sentence ranks with uniform probability\n",
    "    sentence_ranks = np.ones(matrix.shape[0]) / matrix.shape[0]\n",
    "    # Set the damping factor\n",
    "    damping_factor = 0.85\n",
    "    # Set the tolerance\n",
    "    tolerance = 1e-4\n",
    "    # Run the PageRank algorithm until convergence\n",
    "    while True:\n",
    "        next_sentence_ranks = (1 - damping_factor) + damping_factor * transition_matrix.T.dot(sentence_ranks)\n",
    "        diff = np.abs(sentence_ranks - next_sentence_ranks)\n",
    "        if diff.max() < tolerance:\n",
    "            break\n",
    "        sentence_ranks = next_sentence_ranks\n",
    "    return sentence_ranks\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "fHlGBviTEfJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pos_summary = summarize(pos_string,3)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "57PFdAyrEfJk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos_summary"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2EjKwl7BEfJk",
    "outputId": "381309b8-3a89-4241-8635-3df79e54e111"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_summary = summarize(neg_string,3)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PnKnWLhREfJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_summary"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6_5ha4O0EfJk",
    "outputId": "0ab529b7-cdaf-4884-faa0-4fa95febea76"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#open text file\n",
    "text_file = open(\"/Users/bilge/Desktop/pos_summarized.txt\", \"w\")\n",
    "\n",
    "#write string to file\n",
    "text_file.write(pos_summary)\n",
    "\n",
    "#close file\n",
    "text_file.close()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "GbAvuJcIEfJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#open text file\n",
    "text_file = open(\"/Users/bilge/Desktop/neg_summarized.txt\", \"w\")\n",
    "\n",
    "#write string to file\n",
    "text_file.write(neg_summary)\n",
    "\n",
    "#close file\n",
    "text_file.close()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rzX_0fWLEfJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "BHq2awBBEfJk"
   }
  }
 ]
}